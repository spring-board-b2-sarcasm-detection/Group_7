{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba0bb9-c66a-4da0-9c37-e1c4e69fca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "96                |96                |units\n",
      "2                 |2                 |num_layers\n",
      "128               |128               |lstm_units_0\n",
      "0.4               |0.4               |dropout_0\n",
      "64                |64                |final_lstm_units\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shubham\\anaconda4\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1442s\u001b[0m 7s/step - accuracy: 0.5104 - loss: 0.7075 - val_accuracy: 0.5163 - val_loss: 0.6925\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8581s\u001b[0m 43s/step - accuracy: 0.5134 - loss: 0.6968 - val_accuracy: 0.5300 - val_loss: 0.6916\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 6s/step - accuracy: 0.5040 - loss: 0.6952 - val_accuracy: 0.5325 - val_loss: 0.6872\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 6s/step - accuracy: 0.5037 - loss: 0.6960 - val_accuracy: 0.5375 - val_loss: 0.6837\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1049s\u001b[0m 5s/step - accuracy: 0.5223 - loss: 0.6927 - val_accuracy: 0.5269 - val_loss: 0.6880\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1058s\u001b[0m 5s/step - accuracy: 0.5162 - loss: 0.6914 - val_accuracy: 0.5387 - val_loss: 0.6860\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1401s\u001b[0m 7s/step - accuracy: 0.5171 - loss: 0.6942 - val_accuracy: 0.5069 - val_loss: 0.6934\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1912s\u001b[0m 10s/step - accuracy: 0.4984 - loss: 0.6948 - val_accuracy: 0.4931 - val_loss: 0.6934\n",
      "Epoch 9/10\n",
      "\u001b[1m182/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2:33\u001b[0m 9s/step - accuracy: 0.5047 - loss: 0.6946"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_balanced_dataset_FINAL.csv')\n",
    "\n",
    "# Handle missing values in the 'comment' column\n",
    "data['comment'].fillna('', inplace=True)\n",
    "\n",
    "# Reduce dataset size for memory efficiency (sample 10,000 records)\n",
    "data = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Encode target labels if necessary\n",
    "label_column = 'label'\n",
    "label_encoder = LabelEncoder()\n",
    "data[label_column] = label_encoder.fit_transform(data[label_column])\n",
    "\n",
    "# Text Vectorization using TF-IDF with fewer features\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(data['comment']).toarray()\n",
    "\n",
    "# Split data into features and target\n",
    "y = data[label_column]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the hypermodel\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32), return_sequences=True)))\n",
    "    \n",
    "    # Additional LSTM layers with dropout and batch normalization\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Bidirectional(LSTM(units=hp.Int(f'lstm_units_{i}', min_value=32, max_value=128, step=32), return_sequences=True)))\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=hp.Int('final_lstm_units', min_value=32, max_value=128, step=32))))\n",
    "    \n",
    "    # Dense layer with sigmoid activation\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='hyperband',\n",
    "    project_name='bidirectional_lstm_optimization'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = build_model(best_hps)\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Reshape the data for LSTM input\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_rnn, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = (model.predict(X_test_rnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Function to preprocess and predict new input\n",
    "def preprocess_and_predict(comment):\n",
    "    # Preprocess the input comment\n",
    "    input_vector = tfidf.transform([comment]).toarray()\n",
    "    input_vector = scaler.transform(input_vector)\n",
    "    input_vector = input_vector.reshape(input_vector.shape[0], input_vector.shape[1], 1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = (model.predict(input_vector) > 0.5).astype(\"int32\")\n",
    "    return prediction\n",
    "\n",
    "# Example usage for new input\n",
    "new_comment = \"This is a sample comment for prediction.\"\n",
    "prediction = preprocess_and_predict(new_comment)\n",
    "print(f\"Prediction for new comment: {prediction[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45672f2f-df9d-418c-a4f0-2557b2f5bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
