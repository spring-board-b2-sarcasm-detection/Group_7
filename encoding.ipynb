{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26072394-e898-4f28-8421-e52486f3de8e",
   "metadata": {},
   "source": [
    "# Evaluation of Encoding Methods:\n",
    "## One Hot Encoder:\n",
    "\n",
    "Suitability: Not ideal for text data. It would require creating a separate feature for each unique word in the dataset, leading to a very high-dimensional and sparse matrix, which is not efficient for sarcasm detection.\n",
    "## Label Encoder:\n",
    "\n",
    "Suitability: Not suitable for text data as it converts categorical data into integers. It does not capture the semantic meaning of words and thus is not useful for this task.\n",
    "## TF-IDF:\n",
    "\n",
    "Suitability: Good for text classification tasks. TF-IDF can effectively convert the text data into numerical vectors by capturing the importance of words in each comment relative to the entire dataset. It helps in distinguishing common words from more unique words, which can be useful for detecting sarcasm.\n",
    "## Word2Vec:\n",
    "\n",
    "Suitability: Very suitable. Word2Vec creates dense vector representations of words, capturing their semantic meaning. Since sarcasm often depends on the context and semantics of the words, Word2Vec embeddings can be beneficial for a sarcasm detection model.\n",
    "## Term Frequency Encoder:\n",
    "\n",
    "Suitability: Basic but can be useful. It simply counts the occurrences of each word in a document. While it does not consider the importance of words across documents like TF-IDF, it can still provide a baseline representation of text data.\n",
    "\n",
    "\n",
    "## Recommendations:\n",
    "Primary Recommendation: Word2Vec or similar embeddings (like GloVe or FastText). These embeddings can capture semantic and contextual information, which is crucial for detecting sarcasm that often relies on nuanced meanings.\n",
    "\n",
    "\n",
    "Secondary Recommendation: TF-IDF can be a strong alternative if you prefer traditional feature extraction methods. It can provide useful features for machine learning models by highlighting important words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6ae66-65f1-4d5f-86a9-01533ea1fe96",
   "metadata": {},
   "source": [
    "Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084609f9-d561-407a-aabc-8434b0f9ed16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      1                                               need\n",
       "1      0                               might well milk last\n",
       "2      1                                       ask locktrap\n",
       "3      1  im glad community doesnt make console player f...\n",
       "4      0                                    joke put stitch"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_balanced_dataset_FINAL.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing comments\n",
    "data.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f98f1-21fd-4444-a555-5d941a18eff0",
   "metadata": {},
   "source": [
    "Step 2: Preprocess the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ed82b5-b2b9-42b0-b9f9-d90b3ba2e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\haree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>processed_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "      <td>[need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      1                                               need   \n",
       "1      0                               might well milk last   \n",
       "2      1                                       ask locktrap   \n",
       "3      1  im glad community doesnt make console player f...   \n",
       "4      0                                    joke put stitch   \n",
       "\n",
       "                                   processed_comment  \n",
       "0                                             [need]  \n",
       "1                          [might, well, milk, last]  \n",
       "2                                    [ask, locktrap]  \n",
       "3  [im, glad, community, doesnt, make, console, p...  \n",
       "4                                [joke, put, stitch]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the comment column\n",
    "data['processed_comment'] = data['comment'].apply(preprocess_text)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d296f-05cf-4565-9aed-11138632f369",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "## Encoding Methods\n",
    "One Hot Encoding and Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1a42d3-b24d-41d3-9619-9ae1c883f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>processed_comment</th>\n",
       "      <th>processed_comment_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "      <td>[need]</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "      <td>might well milk last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "      <td>ask locktrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "      <td>joke put stitch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      1                                               need   \n",
       "1      0                               might well milk last   \n",
       "2      1                                       ask locktrap   \n",
       "3      1  im glad community doesnt make console player f...   \n",
       "4      0                                    joke put stitch   \n",
       "\n",
       "                                   processed_comment  \\\n",
       "0                                             [need]   \n",
       "1                          [might, well, milk, last]   \n",
       "2                                    [ask, locktrap]   \n",
       "3  [im, glad, community, doesnt, make, console, p...   \n",
       "4                                [joke, put, stitch]   \n",
       "\n",
       "                               processed_comment_str  \n",
       "0                                               need  \n",
       "1                               might well milk last  \n",
       "2                                       ask locktrap  \n",
       "3  im glad community doesnt make console player f...  \n",
       "4                                    joke put stitch  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed_comment_str'] = data['processed_comment'].apply(lambda x: ' '.join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b71ff9a-b898-4e73-be95-285ae8a71d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# One Hot Encoding\n",
    "onehot_vectorizer = CountVectorizer(binary=True)\n",
    "X_onehot = onehot_vectorizer.fit_transform(data['processed_comment_str'])\n",
    "\n",
    "# Term Frequency\n",
    "tf_vectorizer = CountVectorizer()\n",
    "X_tf = tf_vectorizer.fit_transform(data['processed_comment_str'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1c9bd-8b18-4344-9db1-7408385ba6f6",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "# Create TF-IDF Vectors and Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e1912a-780d-4177-a25e-e26db1cca104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['processed_comment_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770fecaa-2e8f-49e4-987f-ad6f8d03c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.67      0.66     12746\n",
      "           1       0.67      0.64      0.65     12994\n",
      "\n",
      "    accuracy                           0.66     25740\n",
      "   macro avg       0.66      0.66      0.66     25740\n",
      "weighted avg       0.66      0.66      0.66     25740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert processed comments back to strings for TF-IDF Vectorizer\n",
    "data['processed_comment_str'] = data['processed_comment'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf.fit_transform(data['processed_comment_str'])\n",
    "\n",
    "# Train-test split\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Model Performance:\\n\", classification_report(y_test, y_pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef95bb-ed9f-40f2-8e6d-968a201b783f",
   "metadata": {},
   "source": [
    "Step 4: \n",
    "# Create Word2Vec Embeddings and Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3138fbf-251d-4cb7-8d4a-824a95cbfd91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train Word2Vec model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['processed_comment'], vector_size=100, window=5, min_count=2, workers=4)\n",
    "word2vec_model.train(data['processed_comment'], total_examples=word2vec_model.corpus_count, epochs=30)\n",
    "\n",
    "# Function to average Word2Vec embeddings\n",
    "def get_avg_word2vec(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply Word2Vec to each comment\n",
    "vector_size = 100\n",
    "data['word2vec'] = data['processed_comment'].apply(lambda x: get_avg_word2vec(x, word2vec_model, vector_size))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_word2vec = np.array(data['word2vec'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e577d2-0cc0-4d21-b47e-c7946e64bdd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (215301553.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install H:\\\\infosys Springboard internship\\\\gensim-4.3.2-cp312-cp312-win_amd64.whl\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "#!pip install --upgrade pip\n",
    "#!pip install gensim==4.3.2\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#!pip install --force-reinstall gensim==4.3.2\n",
    "#pip install --upgrade pip setuptools\n",
    "#!pip install numpy scipy\n",
    "#!pip install gensim==4.3.2\n",
    "#!pip install gensim\n",
    "pip install H:\\\\infosys Springboard internship\\\\gensim-4.3.2-cp312-cp312-win_amd64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95b8456-c006-418b-aec9-b3f1827c8312",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train Word2Vec model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['processed_comment'], vector_size=100, window=5, min_count=2, workers=4)\n",
    "word2vec_model.train(data['processed_comment'], total_examples=word2vec_model.corpus_count, epochs=30)\n",
    "\n",
    "# Function to average Word2Vec embeddings\n",
    "def get_avg_word2vec(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply Word2Vec to each comment\n",
    "vector_size = 100\n",
    "data['word2vec'] = data['processed_comment'].apply(lambda x: get_avg_word2vec(x, word2vec_model, vector_size))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_word2vec = np.array(data['word2vec'].tolist())\n",
    "\n",
    "# Train-test split\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(X_word2vec, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model_w2v = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\n",
    "print(\"Word2Vec Model Performance:\\n\", classification_report(y_test, y_pred_w2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb23bdf-93f0-44e7-8975-5ebe35c43eb7",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795e183b-51b7-46ca-ad40-cdeea334e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoder (for target variable)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82f78c-204c-4bfb-b0fc-d0a5f896c71d",
   "metadata": {},
   "source": [
    "Step 4: \n",
    "# Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91a156-e893-490b-a552-ce26a70bd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split_data(X, y):\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return classification_report(y_test, y_pred)\n",
    "\n",
    "# Prepare data for model training\n",
    "X_train_onehot, X_test_onehot, y_train, y_test = train_test_split_data(X_onehot, y)\n",
    "X_train_tf, X_test_tf, y_train, y_test = train_test_split_data(X_tf, y)\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split_data(X_tfidf, y)\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split_data(X_word2vec, y)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"One Hot Encoding Model Performance:\\n\", train_evaluate_model(X_train_onehot, X_test_onehot, y_train, y_test))\n",
    "print(\"Term Frequency Model Performance:\\n\", train_evaluate_model(X_train_tf, X_test_tf, y_train, y_test))\n",
    "print(\"TF-IDF Model Performance:\\n\", train_evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test))\n",
    "print(\"Word2Vec Model Performance:\\n\", train_evaluate_model(X_train_w2v, X_test_w2v, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cfde8-c034-4e0a-b58f-706827054db2",
   "metadata": {},
   "source": [
    "## Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61840193-94de-475e-a70e-a46bc0abe1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the performance comparison for each encoding method\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"One Hot Encoding Model Performance:\\n\", train_evaluate_model(X_train_onehot, X_test_onehot, y_train, y_test))\n",
    "print(\"Term Frequency Model Performance:\\n\", train_evaluate_model(X_train_tf, X_test_tf, y_train, y_test))\n",
    "print(\"TF-IDF Model Performance:\\n\", train_evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test))\n",
    "print(\"Word2Vec Model Performance:\\n\", train_evaluate_model(X_train_w2v, X_test_w2v, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ffbed-c4e4-4ceb-8540-215a8ce8c4d6",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "Load the Dataset: Reads the dataset and drops any rows with missing comments.\n",
    "\n",
    "Preprocess the Text Data: Cleans and tokenizes the text data, then converts the processed tokens back into strings.\n",
    "\n",
    "Encoding Methods: Encodes the text data using One Hot Encoding, Term Frequency, TF-IDF, and Word2Vec. Label encoding is used for the target variable.\n",
    "\n",
    "Train and Evaluate Models: Trains and evaluates a Random Forest model using each encoding method.\n",
    "\n",
    "Conclusion and Comparison: Prints the performance comparison for each encoding method.\n",
    "\n",
    "This code will help you determine which feature extraction method works best for your sarcasm detection task when using a Random Forest classifier. Adjust the parameters as needed based on your specific requirements and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1028b-3543-46c1-9a31-6d8de53845e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb28f9-607e-44a0-b688-bdf192640041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
