{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3089947e-c53d-440b-9a4d-f00ddf090ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('balanced_dataset_50000.csv')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = ' '.join([WordNetLemmatizer().lemmatize(word) for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data['comment']\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to strings to handle potential float values\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # Maximum number of words to keep based on frequency\n",
    "maxlen = 100  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a9a22-6b77-4d10-a1a8-9e127d72f03b",
   "metadata": {},
   "source": [
    "# Using BERT for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614db48-7a28-46da-a013-744b300d0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are your data arrays\n",
    "# Tokenize and pad sequences\n",
    "maxlen = 100  # Assuming a maximum sequence length\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text data with BERT\n",
    "def encode_texts(texts, batch_size=8):\n",
    "    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    dataset = torch.utils.data.TensorDataset(tokenized_texts['input_ids'], tokenized_texts['attention_mask'])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load pre-trained BERT model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    model.eval()  # Set evaluation mode\n",
    "    \n",
    "    bert_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Use the [CLS] token representation (first token)\n",
    "            cls_token_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            bert_outputs.append(cls_token_embedding.cpu().numpy())\n",
    "    \n",
    "    bert_embeddings = np.concatenate(bert_outputs, axis=0)\n",
    "    return bert_embeddings\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Reduce batch size during BERT encoding if needed\n",
    "X_train_bert = encode_texts(X_train.tolist(), batch_size=4)\n",
    "X_test_bert = encode_texts(X_test.tolist(), batch_size=4)\n",
    "\n",
    "# Combine TF-IDF and BERT features\n",
    "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_bert))\n",
    "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_bert))\n",
    "\n",
    "# Example of how to use the combined features for training and evaluation\n",
    "# Replace this with your actual model training and evaluation code\n",
    "# Assuming you have labels y_train, y_test\n",
    "# Here, we just print some evaluation metrics as an example\n",
    "print('Training data shape:', X_train_combined.shape)\n",
    "print('Testing data shape:', X_test_combined.shape)\n",
    "\n",
    "# Example classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_combined)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc7061-3147-4c94-99ba-287b0b155870",
   "metadata": {},
   "source": [
    "# Advanced Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3600c-a6b1-451e-93c7-f50b615f6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "rf_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Training accuracy\n",
    "train_predictions = rf_model.predict(X_train_combined)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions)\n",
    "\n",
    "# Test accuracy and precision\n",
    "test_predictions = rf_model.predict(X_test_combined)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print('Random Forest Model')\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Training Precision: {train_precision * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Test Precision: {test_precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d8e94-9079-45ed-b351-15fde2186332",
   "metadata": {},
   "source": [
    "# Random Forest Model with Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309ef9fb-f994-499b-a0db-a06d62983030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model\n",
      "Training Accuracy: 65.60%\n",
      "Training Precision: 74.43%\n",
      "Test Accuracy: 62.49%\n",
      "Test Precision: 70.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68      4892\n",
      "           1       0.70      0.45      0.55      5008\n",
      "\n",
      "    accuracy                           0.62      9900\n",
      "   macro avg       0.65      0.63      0.61      9900\n",
      "weighted avg       0.65      0.62      0.61      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Training accuracy\n",
    "train_predictions = rf_model.predict(X_train_tfidf)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision = precision_score(y_train, train_predictions)\n",
    "\n",
    "# Test accuracy and precision\n",
    "test_predictions = rf_model.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print('Random Forest Model')\n",
    "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Training Precision: {train_precision * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Test Precision: {test_precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb84668-5705-4b6a-8a31-1fbf3f676bfb",
   "metadata": {},
   "source": [
    "# SVM Model with Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196b9871-85cb-40b6-93ce-9b9db80cbb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 1, 'kernel': 'rbf'}\n",
      "Best CV Accuracy: 64.94%\n",
      "\n",
      "SVM Model with Tuning and Epochs (Cross-Validation Folds)\n",
      "Training Accuracy: 88.35%\n",
      "Training Precision: 90.35%\n",
      "Test Accuracy: 65.22%\n",
      "Test Precision: 67.61%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.71      0.67      4892\n",
      "           1       0.68      0.60      0.64      5008\n",
      "\n",
      "    accuracy                           0.65      9900\n",
      "   macro avg       0.65      0.65      0.65      9900\n",
      "weighted avg       0.65      0.65      0.65      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IMPROVED\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# SVM Model with GridSearchCV for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf']  # Kernel type\n",
    "}\n",
    "\n",
    "# Number of cross-validation folds (epochs)\n",
    "num_epochs = 5\n",
    "\n",
    "svm_model = GridSearchCV(SVC(random_state=42), param_grid, cv=num_epochs, scoring='accuracy')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best parameters and best score from grid search\n",
    "print(\"Best Parameters: \", svm_model.best_params_)\n",
    "print(\"Best CV Accuracy: {:.2f}%\".format(svm_model.best_score_ * 100))\n",
    "\n",
    "# Training accuracy and precision\n",
    "train_predictions_svm = svm_model.predict(X_train_tfidf)\n",
    "train_accuracy_svm = accuracy_score(y_train, train_predictions_svm)\n",
    "train_precision_svm = precision_score(y_train, train_predictions_svm)\n",
    "\n",
    "# Test accuracy and precision\n",
    "test_predictions_svm = svm_model.predict(X_test_tfidf)\n",
    "test_accuracy_svm = accuracy_score(y_test, test_predictions_svm)\n",
    "test_precision_svm = precision_score(y_test, test_predictions_svm)\n",
    "\n",
    "# Print results\n",
    "print('\\nSVM Model with Tuning and Epochs (Cross-Validation Folds)')\n",
    "print(f'Training Accuracy: {train_accuracy_svm * 100:.2f}%')\n",
    "print(f'Training Precision: {train_precision_svm * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy_svm * 100:.2f}%')\n",
    "print(f'Test Precision: {test_precision_svm * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, test_predictions_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712af64-78fa-4c8f-818a-9a42bd13a5b2",
   "metadata": {},
   "source": [
    "# logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6b4283-9712-4791-bfbe-61998c261c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'lr__C': 0.001, 'lr__penalty': 'l2'}\n",
      "\n",
      "Logistic Regression Model\n",
      "Accuracy: 52.34%\n",
      "Precision: 51.91%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.26      0.35      4892\n",
      "           1       0.52      0.79      0.63      5008\n",
      "\n",
      "    accuracy                           0.52      9900\n",
      "   macro avg       0.53      0.52      0.49      9900\n",
      "weighted avg       0.53      0.52      0.49      9900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.51851095        nan 0.51795539        nan 0.51805643\n",
      "        nan 0.51793015        nan 0.51795541        nan 0.51795541]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'lr__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'lr__penalty': ['l1', 'l2']  # Penalty norm\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train_pad, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "# Evaluate the model with best parameters\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "y_pred_lr = best_lr_model.predict(X_test_pad)\n",
    "\n",
    "# Print evaluation metrics in percentage format\n",
    "print('Logistic Regression Model')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_lr) * 100:.2f}%')\n",
    "print(f'Precision: {precision_score(y_test, y_pred_lr) * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d074b6-1bf6-4261-8c53-d7b87530a457",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e81e8d-7452-4c1f-a2c9-e49298101058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 71ms/step - accuracy: 0.5844 - loss: 0.6650 - val_accuracy: 0.6684 - val_loss: 0.6062\n",
      "Epoch 2/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 71ms/step - accuracy: 0.7210 - loss: 0.5502 - val_accuracy: 0.6518 - val_loss: 0.6266\n",
      "Epoch 3/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.7785 - loss: 0.4735 - val_accuracy: 0.6442 - val_loss: 0.6670\n",
      "Epoch 4/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 71ms/step - accuracy: 0.8049 - loss: 0.4214 - val_accuracy: 0.6354 - val_loss: 0.7370\n",
      "Epoch 5/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.8298 - loss: 0.3675 - val_accuracy: 0.6313 - val_loss: 0.8380\n",
      "Epoch 6/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.8522 - loss: 0.3167 - val_accuracy: 0.6268 - val_loss: 0.9569\n",
      "Epoch 7/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 71ms/step - accuracy: 0.8668 - loss: 0.2820 - val_accuracy: 0.6177 - val_loss: 1.1198\n",
      "Epoch 8/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.8807 - loss: 0.2496 - val_accuracy: 0.6184 - val_loss: 1.3221\n",
      "Epoch 9/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.8968 - loss: 0.2180 - val_accuracy: 0.6159 - val_loss: 1.3789\n",
      "Epoch 10/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 71ms/step - accuracy: 0.9057 - loss: 0.2028 - val_accuracy: 0.6129 - val_loss: 1.6583\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "LSTM Model\n",
      "Accuracy: 61.74%\n",
      "Precision: 61.95%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.60      0.61      4892\n",
      "           1       0.62      0.63      0.63      5008\n",
      "\n",
      "    accuracy                           0.62      9900\n",
      "   macro avg       0.62      0.62      0.62      9900\n",
      "weighted avg       0.62      0.62      0.62      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Define and compile the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "history = lstm_model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "y_pred_lstm = (lstm_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_lstm)\n",
    "precision = precision_score(y_test, y_pred_lstm)\n",
    "\n",
    "# Print results in percentage\n",
    "print('LSTM Model')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e2cfd-d11b-48e1-8785-563a570bdb68",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65316cfc-c40d-400f-9994-db98e2783deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5002 - loss: 41.9204 - val_accuracy: 0.5119 - val_loss: 0.6985\n",
      "Epoch 2/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5064 - loss: 0.9433 - val_accuracy: 0.4939 - val_loss: 0.6935\n",
      "Epoch 3/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5071 - loss: 0.7586 - val_accuracy: 0.5051 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5095 - loss: 0.7129 - val_accuracy: 0.5033 - val_loss: 0.6927\n",
      "Epoch 5/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5116 - loss: 0.7130 - val_accuracy: 0.5038 - val_loss: 0.6918\n",
      "Epoch 6/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5126 - loss: 0.7043 - val_accuracy: 0.5038 - val_loss: 0.6913\n",
      "Epoch 7/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5087 - loss: 0.6983 - val_accuracy: 0.5038 - val_loss: 0.6909\n",
      "Epoch 8/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5087 - loss: 0.6949 - val_accuracy: 0.5038 - val_loss: 0.6906\n",
      "Epoch 9/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5095 - loss: 0.6964 - val_accuracy: 0.5040 - val_loss: 0.6902\n",
      "Epoch 10/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5068 - loss: 0.6938 - val_accuracy: 0.5040 - val_loss: 0.6903\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step\n",
      "Neural Network Model\n",
      "Accuracy: 51.58%\n",
      "Precision: 51.11%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.04      0.08      4892\n",
      "           1       0.51      0.98      0.67      5008\n",
      "\n",
      "    accuracy                           0.52      9900\n",
      "   macro avg       0.59      0.51      0.37      9900\n",
      "weighted avg       0.59      0.52      0.38      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Define Neural Network Model\n",
    "nn_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(maxlen,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = (nn_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "precision = precision_score(y_test, y_pred_nn)\n",
    "\n",
    "# Print results in percentage\n",
    "print('Neural Network Model')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba36f17-d882-435f-b068-aabac6f752f2",
   "metadata": {},
   "source": [
    "# GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5307e95-fe58-4d55-9a88-e9992da2a87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 73ms/step - accuracy: 0.5796 - loss: 0.6688 - val_accuracy: 0.6513 - val_loss: 0.6206\n",
      "Epoch 2/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7265 - loss: 0.5465 - val_accuracy: 0.6455 - val_loss: 0.6263\n",
      "Epoch 3/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7650 - loss: 2021.2390 - val_accuracy: 0.6033 - val_loss: 0.7230\n",
      "Epoch 4/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7468 - loss: 0.5163 - val_accuracy: 0.6045 - val_loss: 0.7281\n",
      "Epoch 5/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7776 - loss: 0.4743 - val_accuracy: 0.6056 - val_loss: 0.7433\n",
      "Epoch 6/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7986 - loss: 0.4480 - val_accuracy: 0.6063 - val_loss: 0.7576\n",
      "Epoch 7/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7957 - loss: 11990.1279 - val_accuracy: 0.5735 - val_loss: 0.7913\n",
      "Epoch 8/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7612 - loss: 0.4791 - val_accuracy: 0.5934 - val_loss: 0.8033\n",
      "Epoch 9/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.7979 - loss: 0.4272 - val_accuracy: 0.5982 - val_loss: 0.8169\n",
      "Epoch 10/10\n",
      "\u001b[1m557/557\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.8126 - loss: 0.4075 - val_accuracy: 0.5955 - val_loss: 0.8289\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
      "GRU Model\n",
      "Accuracy: 60.31%\n",
      "Precision: 62.43%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.67      0.62      4892\n",
      "           1       0.62      0.54      0.58      5008\n",
      "\n",
      "    accuracy                           0.60      9900\n",
      "   macro avg       0.61      0.60      0.60      9900\n",
      "weighted avg       0.61      0.60      0.60      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Assuming you have X_train_pad, X_test_pad, y_train, and y_test ready\n",
    "\n",
    "# Create the GRU model\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128),\n",
    "    GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Increased number of epochs\n",
    "batch_size = 64\n",
    "validation_split = 0.1\n",
    "\n",
    "history = gru_model.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_gru = (gru_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_gru)\n",
    "precision = precision_score(y_test, y_pred_gru)\n",
    "\n",
    "# Print results in percentage\n",
    "print('GRU Model')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_gru))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975d95c-75ff-4677-a07d-ffcabcdcfdea",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebffd2e9-5b91-4d5c-bb4c-e00c6c240373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "Accuracy: 54.57%\n",
      "Precision: 54.12%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.48      4892\n",
      "           1       0.54      0.67      0.60      5008\n",
      "\n",
      "    accuracy                           0.55      9900\n",
      "   macro avg       0.55      0.54      0.54      9900\n",
      "weighted avg       0.55      0.55      0.54      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Decision Tree Model\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=5, min_samples_leaf=5)\n",
    "dt_model.fit(X_train_pad, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_dt = dt_model.predict(X_test_pad)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt)\n",
    "\n",
    "# Print results in percentage\n",
    "print('Decision Tree Model')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ed5a3-5c12-4310-b62f-ce2a390a2af3",
   "metadata": {},
   "source": [
    "# XG BOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18261b2f-fe2b-41b7-aaf3-53cd32fedc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model\n",
      "Accuracy: 56.60%\n",
      "Precision: 56.94%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.56      4892\n",
      "           1       0.57      0.58      0.58      5008\n",
      "\n",
      "    accuracy                           0.57      9900\n",
      "   macro avg       0.57      0.57      0.57      9900\n",
      "weighted avg       0.57      0.57      0.57      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# XGBoost Model with initial parameters\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_pad, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_xgb = xgb_model.predict(X_test_pad)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "precision = precision_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print results in percentage\n",
    "print('XGBoost Model')\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44ac48-f9c7-4e18-a6ff-682173d9b5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
