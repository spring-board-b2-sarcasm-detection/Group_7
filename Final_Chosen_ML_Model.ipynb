{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c98d68-f140-4bb4-aec0-49cf8eeb7f69",
   "metadata": {},
   "source": [
    "# SVC Model\n",
    "\n",
    "\n",
    "When considering whether to use `SVC` (Support Vector Classifier) from `sklearn.svm` for sarcasm detection, here are some key reasons to prefer `SVC` over other approaches within the context of sarcasm detection:\n",
    "\n",
    "### Reasons to Use `SVC`:\n",
    "\n",
    "1. **Effectiveness in High-Dimensional Spaces:**\n",
    "   `SVC` is known for its effectiveness in high-dimensional spaces, which is particularly relevant when dealing with text data that has been transformed into TF-IDF features. The high dimensionality of the feature space can be efficiently handled by the SVC algorithm.\n",
    "\n",
    "2. **Robustness to Overfitting:**\n",
    "   SVMs, including `SVC`, have regularization parameters (`C` in `SVC`) that help prevent overfitting. This is crucial for sarcasm detection, where the model needs to generalize well to new, unseen examples of sarcasm without being too specific to the training data.\n",
    "\n",
    "3. **Kernel Trick:**\n",
    "   `SVC` supports the use of kernel functions, which can map the original features into higher-dimensional spaces where a linear separation is possible. This is beneficial for sarcasm detection, as the relationship between features and labels may not be linearly separable in the original feature space. Using kernels like the RBF (Radial Basis Function) can capture complex patterns in the data.\n",
    "\n",
    "4. **Performance:**\n",
    "   Empirical evidence often shows that `SVC` performs well in text classification tasks, including sarcasm detection. The `SVC` algorithm can achieve high accuracy and precision, making it a strong candidate for this type of task.\n",
    "\n",
    "5. **Scalability:**\n",
    "   While SVMs can be computationally intensive, `SVC` implementations in `scikit-learn` are optimized for performance. They can handle reasonably large datasets efficiently, especially when combined with techniques like grid search for hyperparameter tuning.\n",
    "\n",
    "### Comparison with Other Approaches:\n",
    "\n",
    "- **Logistic Regression:** While logistic regression is simpler and faster, it might not capture the complex patterns in sarcasm detection as effectively as `SVC` with non-linear kernels.\n",
    "- **Decision Trees/Random Forests:** These models can capture non-linear patterns but may require extensive tuning and can be prone to overfitting. They also may not perform as well in high-dimensional spaces compared to `SVC`.\n",
    "- **XGBoost:** While powerful and often performing well in various tasks, XGBoost might be more complex to tune and more computationally intensive compared to `SVC`.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "**SVC** is a strong candidate for sarcasm detection due to its ability to handle high-dimensional feature spaces, robustness against overfitting, support for non-linear classification through kernel functions, and demonstrated empirical performance in text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c91617-ffa3-403f-b1d7-ffe751d19fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 50000\n",
      "Shape of the data: (50000, 2)\n",
      "Null values:\n",
      "label        0\n",
      "comment    502\n",
      "dtype: int64\n",
      "Value counts of target column:\n",
      "comment\n",
      "forgot                                                                                              280\n",
      "dropped                                                                                             118\n",
      "yes                                                                                                  81\n",
      "thanks                                                                                               64\n",
      "lol                                                                                                  50\n",
      "                                                                                                   ... \n",
      "joke u part backgammon operation                                                                      1\n",
      "parachute stage ignition                                                                              1\n",
      "well hows wife holding                                                                                1\n",
      "seen several time live popualtion people could person could several different people way knowing      1\n",
      "idk dude dft pretty op malphite                                                                       1\n",
      "Name: count, Length: 46780, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'balanced_dataset_50000.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Count the number of samples\n",
    "num_samples = len(data)\n",
    "print(f'Number of samples: {num_samples}')\n",
    "\n",
    "# Get the shape of the data\n",
    "data_shape = data.shape\n",
    "print(f'Shape of the data: {data_shape}')\n",
    "\n",
    "# Count null values\n",
    "null_values = data.isnull().sum()\n",
    "print(f'Null values:\\n{null_values}')\n",
    "\n",
    "# Remove null values\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Shuffle the dataset\n",
    "data_shuffled = data_cleaned.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the new dataset\n",
    "new_file_path = 'balanced_dataset_50000_cleaned_shuffled.csv'\n",
    "data_shuffled.to_csv(new_file_path, index=False)\n",
    "\n",
    "# Count the values of 0s and 1s (assuming the target column is the last column)\n",
    "value_counts = data_shuffled.iloc[:, -1].value_counts()\n",
    "print(f'Value counts of target column:\\n{value_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552149aa-fb5e-4659-9ffc-853fa17663fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts in the label column:\n",
      "label\n",
      "1    24908\n",
      "0    24590\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the values of 0s and 1s in the 'label' column\n",
    "label_counts = data_shuffled['label'].value_counts()\n",
    "print(f'Value counts in the label column:\\n{label_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cee298-821d-42d2-a07d-abb4df3130c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('balanced_dataset_50000_cleaned_shuffled.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data['comment']\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to strings to handle potential float values\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)\n",
    "\n",
    "# Tokenize and transform sequences using TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20bdb59-2134-403d-9cf8-4f69883222d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            comment\n",
      "0      0                                      yes joke sure\n",
      "1      0  credit worthiness isnt going factor dealer dec...\n",
      "2      0                         m paint motion adobe flash\n",
      "3      0                               take pantsand jacket\n",
      "4      0                      cobalt alchemist crimson oems\n",
      "5      0                                           guy fuck\n",
      "6      0  seriously neither option ballot option win one...\n",
      "7      1                                   wouldnt happened\n",
      "8      0            german car french wine imagine response\n",
      "9      1  used build house couldwould take month complet...\n"
     ]
    }
   ],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1e289e-8a82-4e85-a50b-1894efc8dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49498, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe435d4-8a1a-4757-9d8d-432a4c427000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (49498, 2)\n",
      "X_train shape: (39598,)\n",
      "y_train shape: (39598,)\n",
      "X_test shape: (9900,)\n",
      "y_test shape: (9900,)\n",
      "X_train_tfidf shape: (39598, 10000)\n",
      "X_test_tfidf shape: (9900, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original dataset shape:\", data.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test_tfidf shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9711fd27-0dae-4174-8126-1a6b00479d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.812625\n",
      "Testing Accuracy: 0.622\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.64       988\n",
      "           1       0.64      0.57      0.60      1012\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.62      0.62      0.62      2000\n",
      "weighted avg       0.62      0.62      0.62      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the cleaned and shuffled dataset\n",
    "file_path = 'balanced_dataset_50000_cleaned_shuffled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Subsample the dataset for quicker experimentation (e.g., using 10,000 samples)\n",
    "data_sampled = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data_sampled['comment']  # Assuming 'comment' column contains text data\n",
    "y = data_sampled['label']\n",
    "\n",
    "# Convert text data to TF-IDF features with limited number of features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Classifier with a linear kernel\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = svc.predict(X_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_test_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Testing Accuracy: {test_accuracy}')\n",
    "print(f'Classification Report:\\n{test_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b892b5-685f-47de-8514-cfea40989be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc16590d-0908-4871-ba21-5ede52f8d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Parameters: {'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Training Accuracy: 0.94675\n",
      "Testing Accuracy: 0.634\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.69      0.65       988\n",
      "           1       0.66      0.58      0.62      1012\n",
      "\n",
      "    accuracy                           0.63      2000\n",
      "   macro avg       0.64      0.63      0.63      2000\n",
      "weighted avg       0.64      0.63      0.63      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the cleaned and shuffled dataset\n",
    "file_path = 'balanced_dataset_50000_cleaned_shuffled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Subsample the dataset for quicker experimentation (e.g., using 10,000 samples)\n",
    "data_sampled = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = data_sampled['comment']  # Assuming 'comment' column contains text data\n",
    "y = data_sampled['label']\n",
    "\n",
    "# Convert text data to TF-IDF features with limited number of features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Classifier with a linear kernel\n",
    "svc = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [3, 4, 5],  # Only applicable for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto']  # Only applicable for 'rbf', 'poly', and 'sigmoid'\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Train the model with the best estimator\n",
    "best_svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = best_svc.predict(X_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_test_pred = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Testing Accuracy: {test_accuracy}')\n",
    "print(f'Classification Report:\\n{test_report}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f136f-06f5-4b1a-b7ff-a9acdd4cd684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bc50da-ca8e-41a7-96b9-2bdd0f85210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15237046-257b-4885-967c-975c6cd25266",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "train_predictions = best_model.predict(X_train)\n",
    "test_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3583d09f-8af2-4058-814e-34e91309b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 94.67%\n",
      "Test Accuracy: 63.40%\n",
      "Test Precision: 65.66%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy: {:.2f}%\".format(accuracy_score(y_train, train_predictions) * 100))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(accuracy_score(y_test, test_predictions) * 100))\n",
    "print(\"Test Precision: {:.2f}%\".format(precision_score(y_test, test_predictions) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae86f45-b5f2-4332-9b41-4f0bcdc3f0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89db42c-99da-4728-a4b3-56ba867c2e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba3c0f-a439-49ee-8323-6924cdb889cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76a426-fe40-49cd-940f-833f9937fb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b37de68c-21ed-4075-932f-38fd96a938f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9644426486186171\n",
      "Testing Accuracy: 0.6441414141414141\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65      4975\n",
      "           1       0.64      0.64      0.64      4925\n",
      "\n",
      "    accuracy                           0.64      9900\n",
      "   macro avg       0.64      0.64      0.64      9900\n",
      "weighted avg       0.64      0.64      0.64      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'balanced_dataset_50000_cleaned_shuffled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = data['comment']\n",
    "y = data['label']\n",
    "\n",
    "# Convert text data to TF-IDF features with limited number of features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert text data to count features with limited number of features\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = count_vectorizer.fit_transform(X)\n",
    "\n",
    "# Combine the TF-IDF and count features\n",
    "X_features = hstack([X_tfidf, X_count])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Testing Accuracy: {test_accuracy}')\n",
    "print(f'Classification Report:\\n{test_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3525cf76-36c6-4824-86e1-34f66fc81f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "filename = 'trained_model.sav'\n",
    "pickle.dump(best_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Input text\n",
    "input_text = \"I'm thrilled to spend my weekend working.\"\n",
    "\n",
    "# Preprocess the input text\n",
    "input_tfidf = vectorizer.transform([input_text])\n",
    "\n",
    "# Predict the sentiment\n",
    "prediction = loaded_model.predict(input_tfidf)[0]\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Sentiment:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5385c07-49b8-4603-b26a-36c15c501817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "filename = 'trained_model.sav'\n",
    "pickle.dump(best_model, open(filename, 'wb'))\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Input text\n",
    "input_text = \"wouldnt happened\"\n",
    "\n",
    "# Preprocess the input text\n",
    "input_tfidf = vectorizer.transform([input_text])\n",
    "\n",
    "# Predict the sentiment\n",
    "prediction = loaded_model.predict(input_tfidf)[0]\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Sentiment:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926d50be-9732-4c1d-8300-a025b964b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 0: Non-sarcastic\n",
      "Comment 1: Non-sarcastic\n",
      "Comment 2: Non-sarcastic\n",
      "Comment 3: Sarcastic\n",
      "Comment 4: Sarcastic\n",
      "Comment 5: Non-sarcastic\n",
      "Comment 6: Non-sarcastic\n",
      "Comment 7: Sarcastic\n",
      "Comment 8: Non-sarcastic\n",
      "Comment 9: Non-sarcastic\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved model\n",
    "filename = 'trained_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Input texts\n",
    "input_texts = [\n",
    "    \"I just love waiting in long lines. It's my favorite pastime.\",\n",
    "    \"Wow, another rainy day. Just what I needed.\",\n",
    "    \"I'm so excited for Monday morning meetings.\",\n",
    "    \"The food here is amazing, said no one ever.\",\n",
    "    \"I'm thrilled to spend my weekend working.\",\n",
    "    \"What a beautiful day to stay indoors.\",\n",
    "    \"I can't wait to do my taxes. It's so much fun.\",\n",
    "    \"Great, my phone battery died in the middle of nowhere.\",\n",
    "    \"I absolutely love getting stuck in traffic for hours.\",\n",
    "    \"Thank you for the wonderful gift. Itâ€™s exactly what I didn't want.\"\n",
    "]\n",
    "\n",
    "# Preprocess the input texts\n",
    "input_tfidf = vectorizer.transform(input_texts)\n",
    "\n",
    "# Predict the sentiment\n",
    "predictions = loaded_model.predict(input_tfidf)\n",
    "\n",
    "# Print the predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction == 0:\n",
    "        print(f\"Comment {i}: Non-sarcastic\")\n",
    "    else:\n",
    "        print(f\"Comment {i}: Sarcastic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5875c98b-839d-400e-81d8-f2b1d40a0599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 0: Sarcastic\n",
      "Comment 1: Non-sarcastic\n",
      "Comment 2: Non-sarcastic\n",
      "Comment 3: Sarcastic\n",
      "Comment 4: Sarcastic\n",
      "Comment 5: Sarcastic\n",
      "Comment 6: Non-sarcastic\n",
      "Comment 7: Sarcastic\n",
      "Comment 8: Non-sarcastic\n",
      "Comment 9: Sarcastic\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved model\n",
    "filename = 'trained_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Input texts\n",
    "input_texts = [\n",
    "    \"The weather looks great today. Perfect for a picnic!\",\n",
    "    \"Wow, another rainy day. Just what I needed.\",\n",
    "    \"Thank you for considering my application. I look forward to hearing from you.\",\n",
    "    \"The food here is amazing, said no one ever.\",\n",
    "    \"I'm thrilled to spend my weekend working.\",\n",
    "    \"I appreciate your feedback on the project. Let's discuss it further.\",\n",
    "    \"I can't wait to do my taxes. It's so much fun.\",\n",
    "    \"Great, my phone battery died in the middle of nowhere.\",\n",
    "    \"I absolutely love getting stuck in traffic for hours.\",\n",
    "    \"I enjoyed our conversation yesterday. Let's catch up again soon.\"\n",
    "]\n",
    "\n",
    "# Preprocess the input texts\n",
    "input_tfidf = vectorizer.transform(input_texts)\n",
    "\n",
    "# Predict the sentiment\n",
    "predictions = loaded_model.predict(input_tfidf)\n",
    "\n",
    "# Print the predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction == 0:\n",
    "        print(f\"Comment {i}: Non-sarcastic\")\n",
    "    else:\n",
    "        print(f\"Comment {i}: Sarcastic\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6043c-0984-4a86-8f26-9d8fcaf5a6a8",
   "metadata": {},
   "source": [
    "List of points highlighting why a deep learning (DL) model is preferred over a Support Vector Classifier (SVC) for sarcasm detection:\n",
    "\n",
    "1. **Automatic Feature Learning:** DL models automatically learn relevant features from raw text data, eliminating the need for manual feature engineering required by SVCs.\n",
    "\n",
    "2. **Handling Non-linear Relationships:** DL models can capture complex, non-linear relationships in language, which are crucial for understanding nuanced sarcasm expressions. SVCs, being linear classifiers, may struggle with such complexities.\n",
    "\n",
    "3. **Contextual Understanding:** DL models, especially those with attention mechanisms, excel at capturing contextual dependencies across sentences, enhancing their ability to recognize sarcasm based on broader linguistic context.\n",
    "\n",
    "4. **Scalability with Large Datasets:** DL models scale effectively with large datasets, crucial for sarcasm detection which involves diverse and subtle forms of expression. SVCs may not generalize well with increasing data complexity.\n",
    "\n",
    "5. **Utilization of Pretrained Models:** DL models can leverage pretrained language models like BERT or GPT, which encode extensive linguistic knowledge. Fine-tuning these models for sarcasm detection improves accuracy and robustness compared to SVCs.\n",
    "\n",
    "6. **Adaptability to Domain-Specific Contexts:** DL models can be fine-tuned on domain-specific datasets, allowing them to better understand and predict sarcasm within specific contexts, whereas SVCs may struggle to generalize beyond initial training domains.\n",
    "\n",
    "7. **Continuous Learning:** DL models can be updated with new data continuously, improving over time as language use and sarcasm expressions evolve. SVCs typically require retraining from scratch with updated datasets, which is more resource-intensive.\n",
    "\n",
    "In summary, DL models offer advantages in automatic feature learning, handling complex relationships, contextual understanding, scalability with large datasets, utilization of pretrained models, adaptability to domains, and continuous learning, making them more effective than SVCs for sarcasm detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa656a3-48eb-478c-ba72-335edb3a11fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
