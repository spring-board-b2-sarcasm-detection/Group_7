{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df29ddb6-a1d7-4415-83b1-9f01b8100166",
   "metadata": {},
   "source": [
    "The code aims to load a text dataset, preprocess it, and compare the performance of four different deep learning models by evaluating their F1 scores. The models compared are:\n",
    "\n",
    "Bidirectional LSTM\n",
    "GRU\n",
    "1D Convolutional LSTM\n",
    "Attention-based LSTM\n",
    "Each model is trained on a subset of the data for 2 epochs, and their performance is evaluated using the F1 score on a test set.\n",
    "\n",
    "Step-by-Step Breakdown\n",
    "1. Importing Necessary Libraries\n",
    "The code starts by importing libraries for data processing, model building, training, and evaluation. These include pandas, numpy, scikit-learn, and TensorFlow with Keras.\n",
    "\n",
    "2. Loading and Preprocessing the Data\n",
    "Loading the Dataset: The dataset is loaded from a CSV file.\n",
    "Handling Missing Values: Missing values in the 'comment' column are filled with an empty string to ensure no null values are present.\n",
    "Reducing Dataset Size: To manage memory constraints, a random sample of 10,000 records is taken from the dataset to make the computations more feasible.\n",
    "Encoding Target Labels: The target labels are encoded using LabelEncoder to convert categorical labels into numerical format.\n",
    "Text Vectorization: The text data in the 'comment' column is vectorized using TF-IDF (Term Frequency-Inverse Document Frequency) with a maximum of 1000 features to convert text into numerical vectors.\n",
    "Splitting the Data: The dataset is split into training and testing sets to evaluate the models' performance on unseen data.\n",
    "Standardizing the Data: The feature data is standardized using StandardScaler to have zero mean and unit variance, which helps in faster convergence during training.\n",
    "\n",
    "3. Defining the Models\n",
    "Bidirectional LSTM: This model uses Bidirectional LSTM layers to capture information from both forward and backward sequences. It is particularly useful for sequence data where context from both directions can be beneficial.\n",
    "GRU (Gated Recurrent Unit): This model uses GRU layers, which are an alternative to LSTM with fewer parameters, making it faster and sometimes more efficient.\n",
    "1D Convolutional LSTM: This model combines ConvLSTM2D layers, which apply convolutional operations over sequences, followed by Flatten and Dense layers. It captures spatial and temporal dependencies in the data.\n",
    "Attention-based LSTM: This model uses an LSTM layer followed by a custom attention mechanism. The attention layer helps the model focus on important parts of the sequence by assigning different weights to different parts of the input.\n",
    "\n",
    "4. Reshaping Data for Models\n",
    "ConvLSTM Data Reshaping: The data is reshaped to fit the input requirements of the ConvLSTM2D layer.\n",
    "LSTM, GRU, and Attention Data Reshaping: The data is reshaped to fit the input requirements of LSTM, GRU, and Attention-based LSTM models.\n",
    "\n",
    "5. Training and Evaluating Models\n",
    "Model Training: Each model is trained on the training data for 2 epochs. Training involves feeding the data to the model, calculating the loss, and updating the model weights to minimize the loss.\n",
    "Model Evaluation: After training, each model makes predictions on the test data. These predictions are then compared to the actual labels to calculate the F1 score, which considers both precision and recall. The F1 score is particularly useful for imbalanced datasets.\n",
    "\n",
    "6. Displaying F1 Scores\n",
    "The F1 scores of all models are printed and compared to evaluate their performance. The model with the highest F1 score is considered the best performing model for the given task.\n",
    "\n",
    "\n",
    "The models are compared based on their F1 scores. The F1 score is a measure of a model's accuracy considering both precision (the number of true positive results divided by the number of all positive results, including those not correctly identified) and recall (the number of true positive results divided by the number of positives that should have been identified). This metric is particularly useful for evaluating models on imbalanced datasets, where the number of positive and negative samples may not be equal. By comparing the F1 scores, we can determine which model performs best in terms of balancing precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7673eed7-eec7-4aad-a870-fea30624c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shubham\\anaconda4\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Training Bidirectional LSTM model...\n",
      "Epoch 1/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 708ms/step - accuracy: 0.5107 - loss: 0.6932\n",
      "Epoch 2/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 696ms/step - accuracy: 0.5195 - loss: 0.6917\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 280ms/step\n",
      "Bidirectional LSTM F1 Score: 0.6293706293706294\n",
      "Training GRU model...\n",
      "Epoch 1/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 568ms/step - accuracy: 0.4965 - loss: 0.6938\n",
      "Epoch 2/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 562ms/step - accuracy: 0.5015 - loss: 0.6935\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 203ms/step\n",
      "GRU F1 Score: 0.6574882471457354\n",
      "Training 1D Convolutional LSTM model...\n",
      "Epoch 1/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 161ms/step - accuracy: 0.5643 - loss: 0.6789\n",
      "Epoch 2/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 151ms/step - accuracy: 0.6834 - loss: 0.5972\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step\n",
      "1D Convolutional LSTM F1 Score: 0.5943345804382684\n",
      "Training Attention-based LSTM model...\n",
      "Epoch 1/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 849ms/step - accuracy: 0.5077 - loss: 0.6933\n",
      "Epoch 2/2\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 561ms/step - accuracy: 0.5037 - loss: 0.6934\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step\n",
      "Attention-based LSTM F1 Score: 0.652158142908959\n",
      "F1 Scores for different models:\n",
      "Bidirectional LSTM: 0.6293706293706294\n",
      "GRU: 0.6574882471457354\n",
      "1D Convolutional LSTM: 0.5943345804382684\n",
      "Attention-based LSTM: 0.652158142908959\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Flatten, LSTM, GRU, Bidirectional, ConvLSTM2D, Layer, Activation\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Reshape, Add\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_balanced_dataset_FINAL.csv')\n",
    "\n",
    "# Handle missing values in the 'comment' column\n",
    "data['comment'].fillna('', inplace=True)\n",
    "\n",
    "# Reduce dataset size for memory efficiency (sample 10,000 records)\n",
    "data = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Encode target labels if necessary\n",
    "label_column = 'label'\n",
    "label_encoder = LabelEncoder()\n",
    "data[label_column] = label_encoder.fit_transform(data[label_column])\n",
    "\n",
    "# Text Vectorization using TF-IDF with fewer features\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(data['comment']).toarray()\n",
    "\n",
    "# Split data into features and target\n",
    "y = data[label_column]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define a custom attention layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.matmul(inputs, self.W) + self.b\n",
    "        k = inputs\n",
    "        v = inputs\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        score = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(score, v)\n",
    "        return context\n",
    "\n",
    "# Define the models\n",
    "def build_bilstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(GRU(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(None, input_shape[0], 1, 1)))\n",
    "    model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_attention_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(64, return_sequences=True)(inputs)\n",
    "    attention_out = AttentionLayer()(lstm_out)\n",
    "    attention_out = GlobalAveragePooling1D()(attention_out)\n",
    "    outputs = Dense(1, activation='sigmoid')(attention_out)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshape data for ConvLSTM\n",
    "X_train_conv_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1], 1, 1))\n",
    "X_test_conv_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1], 1, 1))\n",
    "\n",
    "# Reshape data for LSTM, GRU, and Attention models\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define input shapes for different models\n",
    "input_shape_rnn = (X_train.shape[1], 1)\n",
    "input_shape_conv_lstm = (X_train.shape[1],)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Bidirectional LSTM': build_bilstm_model(input_shape_rnn),\n",
    "    'GRU': build_gru_model(input_shape_rnn),\n",
    "    '1D Convolutional LSTM': build_conv_lstm_model(input_shape_conv_lstm),\n",
    "    'Attention-based LSTM': build_attention_lstm_model(input_shape_rnn)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "f1_scores = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    if model_name == '1D Convolutional LSTM':\n",
    "        model.fit(X_train_conv_lstm, y_train, epochs=2, batch_size=32, verbose=1)\n",
    "        y_pred = (model.predict(X_test_conv_lstm) > 0.5).astype(\"int32\")\n",
    "    else:\n",
    "        model.fit(X_train_rnn, y_train, epochs=2, batch_size=32, verbose=1)\n",
    "        y_pred = (model.predict(X_test_rnn) > 0.5).astype(\"int32\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_scores[model_name] = f1\n",
    "    print(f\"{model_name} F1 Score: {f1}\")\n",
    "\n",
    "# Display F1 scores\n",
    "print(\"F1 Scores for different models:\")\n",
    "for model_name, score in f1_scores.items():\n",
    "    print(f\"{model_name}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
