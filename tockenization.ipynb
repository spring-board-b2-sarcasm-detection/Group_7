{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "622450a7-8bef-4227-837f-e6ddeae2232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40a11d35-824a-4abd-bd69-8c10316ac9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\haree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e9627e5-4c84-4e38-aa52-1babdf9e9d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0      1                                               need\n",
       "1      0                               might well milk last\n",
       "2      1                                       ask locktrap\n",
       "3      1  im glad community doesnt make console player f...\n",
       "4      0                                    joke put stitch"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "data = pd.read_csv('cleaned_balanced_dataset_FINAL.csv')\n",
    "\n",
    "# Displaying first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12e45100-785b-4c38-9534-36c98797612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['label', 'comment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the dataset:\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48b8298b-a867-4b1e-b99a-167923a7ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values and converting all entries to strings\n",
    "data['comment'] = data['comment'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4270075d-affa-4da7-8ae2-1f7d55cf04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer):\n",
    "    if tokenizer == 'word':\n",
    "        return word_tokenize(text)\n",
    "    elif tokenizer == 'sentence':\n",
    "        return sent_tokenize(text)\n",
    "    elif tokenizer == 'tweet':\n",
    "        tknzr = TweetTokenizer()\n",
    "        return tknzr.tokenize(text)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown tokenizer. Choose 'word', 'sentence', or 'tweet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24e6c9f2-7045-4db2-a02f-dc8da4691c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>word_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "      <td>[need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      1                                               need   \n",
       "1      0                               might well milk last   \n",
       "2      1                                       ask locktrap   \n",
       "3      1  im glad community doesnt make console player f...   \n",
       "4      0                                    joke put stitch   \n",
       "\n",
       "                                      word_tokenized  \n",
       "0                                             [need]  \n",
       "1                          [might, well, milk, last]  \n",
       "2                                    [ask, locktrap]  \n",
       "3  [im, glad, community, doesnt, make, console, p...  \n",
       "4                                [joke, put, stitch]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing text using word tokenizer\n",
    "data['word_tokenized'] = data['comment'].apply(lambda x: tokenize_text(x, 'word'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35cea03c-6627-4fb8-99e4-03ce5c272f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>word_tokenized</th>\n",
       "      <th>sentence_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "      <td>[need]</td>\n",
       "      <td>[need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "      <td>[might well milk last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "      <td>[ask locktrap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "      <td>[im glad community doesnt make console player ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "      <td>[joke put stitch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      1                                               need   \n",
       "1      0                               might well milk last   \n",
       "2      1                                       ask locktrap   \n",
       "3      1  im glad community doesnt make console player f...   \n",
       "4      0                                    joke put stitch   \n",
       "\n",
       "                                      word_tokenized  \\\n",
       "0                                             [need]   \n",
       "1                          [might, well, milk, last]   \n",
       "2                                    [ask, locktrap]   \n",
       "3  [im, glad, community, doesnt, make, console, p...   \n",
       "4                                [joke, put, stitch]   \n",
       "\n",
       "                                  sentence_tokenized  \n",
       "0                                             [need]  \n",
       "1                             [might well milk last]  \n",
       "2                                     [ask locktrap]  \n",
       "3  [im glad community doesnt make console player ...  \n",
       "4                                  [joke put stitch]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing text using sentence tokenizer\n",
    "data['sentence_tokenized'] = data['comment'].apply(lambda x: tokenize_text(x, 'sentence'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdda1a6b-6ff0-4c62-8f57-d8527af53942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>word_tokenized</th>\n",
       "      <th>sentence_tokenized</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "      <td>[need]</td>\n",
       "      <td>[need]</td>\n",
       "      <td>[need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>might well milk last</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "      <td>[might well milk last]</td>\n",
       "      <td>[might, well, milk, last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ask locktrap</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "      <td>[ask locktrap]</td>\n",
       "      <td>[ask, locktrap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>im glad community doesnt make console player f...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "      <td>[im glad community doesnt make console player ...</td>\n",
       "      <td>[im, glad, community, doesnt, make, console, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>joke put stitch</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "      <td>[joke put stitch]</td>\n",
       "      <td>[joke, put, stitch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0      1                                               need   \n",
       "1      0                               might well milk last   \n",
       "2      1                                       ask locktrap   \n",
       "3      1  im glad community doesnt make console player f...   \n",
       "4      0                                    joke put stitch   \n",
       "\n",
       "                                      word_tokenized  \\\n",
       "0                                             [need]   \n",
       "1                          [might, well, milk, last]   \n",
       "2                                    [ask, locktrap]   \n",
       "3  [im, glad, community, doesnt, make, console, p...   \n",
       "4                                [joke, put, stitch]   \n",
       "\n",
       "                                  sentence_tokenized  \\\n",
       "0                                             [need]   \n",
       "1                             [might well milk last]   \n",
       "2                                     [ask locktrap]   \n",
       "3  [im glad community doesnt make console player ...   \n",
       "4                                  [joke put stitch]   \n",
       "\n",
       "                                     tweet_tokenized  \n",
       "0                                             [need]  \n",
       "1                          [might, well, milk, last]  \n",
       "2                                    [ask, locktrap]  \n",
       "3  [im, glad, community, doesnt, make, console, p...  \n",
       "4                                [joke, put, stitch]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing text using tweet tokenizer\n",
    "data['tweet_tokenized'] = data['comment'].apply(lambda x: tokenize_text(x, 'tweet'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f2268b3-27a9-4380-b03b-e07a369422d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test sets\n",
    "if 'comment' in data.columns and 'label' in data.columns:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['label'], test_size=0.2, random_state=42)\n",
    "else:\n",
    "    raise KeyError(\"The 'comment' or 'label' column is not present in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d12e1df1-6c90-4820-9427-93fb9f968929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and evaluate a Random Forest model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, tokenizer):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(tokenizer=lambda x: tokenize_text(x, tokenizer))),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f\"Results for {tokenizer} tokenizer:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15f578-850e-42ff-8221-5de620327b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8141514-249b-438f-965c-cf90ee36f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for word tokenizer:\n",
      "Accuracy: 0.636314696608475\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     13045\n",
      "           1       0.65      0.60      0.62     12961\n",
      "\n",
      "    accuracy                           0.64     26006\n",
      "   macro avg       0.64      0.64      0.64     26006\n",
      "weighted avg       0.64      0.64      0.64     26006\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8786 4259]\n",
      " [5199 7762]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model using word tokenizer\n",
    "evaluate_model(X_train, y_train, X_test, y_test, 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2807a305-8626-4844-87de-7c84a9ac4dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for sentence tokenizer:\n",
      "Accuracy: 0.5228408828731831\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.98      0.67     13045\n",
      "           1       0.77      0.06      0.11     12961\n",
      "\n",
      "    accuracy                           0.52     26006\n",
      "   macro avg       0.64      0.52      0.39     26006\n",
      "weighted avg       0.64      0.52      0.39     26006\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12810   235]\n",
      " [12174   787]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model using sentence tokenizer\n",
    "evaluate_model(X_train, y_train, X_test, y_test, 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbc6d745-cd19-44cc-a39c-c6721c2ef5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haree\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for tweet tokenizer:\n",
      "Accuracy: 0.6351611166653849\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     13045\n",
      "           1       0.65      0.60      0.62     12961\n",
      "\n",
      "    accuracy                           0.64     26006\n",
      "   macro avg       0.64      0.64      0.63     26006\n",
      "weighted avg       0.64      0.64      0.63     26006\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8805 4240]\n",
      " [5248 7713]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model using tweet tokenizer\n",
    "evaluate_model(X_train, y_train, X_test, y_test, 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b320dfa0-7a5e-43c6-b0e5-d5156c012686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the evaluation, the word tokenizer performs the best for the sarcasm detection task using a Random Forest model.\n"
     ]
    }
   ],
   "source": [
    "# Conclusion\n",
    "print(\"Based on the evaluation, the word tokenizer performs the best for the sarcasm detection task using a Random Forest model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6370b9f-abce-4cc5-9c97-06bb484ac0c9",
   "metadata": {},
   "source": [
    "## Summary \n",
    "Imported Libraries: Loaded necessary libraries for data processing, text tokenization, and machine learning.\n",
    "\n",
    "Loaded Dataset: Read a CSV file containing sarcastic comments and labels, and handled any missing values by converting all entries to strings.\n",
    "\n",
    "Defined Tokenization Function: Created a function to tokenize text using different methods: word tokenization, sentence tokenization, and tweet tokenization.\n",
    "\n",
    "Tokenized Text: Applied the tokenization function to the comments in the dataset, creating new columns for each type of tokenization.\n",
    "\n",
    "Split Data: Divided the dataset into training and test sets with an 80-20 split.\n",
    "\n",
    "Evaluated Model: Built and evaluated a Random Forest model for sarcasm detection using each type of tokenization, printing accuracy, classification reports, and confusion matrices.\n",
    "\n",
    "Conclusion: Determined that the word tokenizer provided the best performance for the sarcasm detection task using the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d378740-c37a-42af-8342-412576ac04bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\haree\\AppData\\Local\\Temp\\ipykernel_16944\\3472294941.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  \"\"\"import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\n\\n# Load dataset\\ndata = pd.read_csv(\\'cleaned_balanced_dataset_FINAL.csv\\')\\n\\n# Preprocessing\\ndata[\\'processed_comment\\'] = data[\\'comment\\'].str.lower().str.replace(\\'[^\\\\w\\\\s]\\', \\'\\')\\ndata[\\'processed_comment_str\\'] = data[\\'processed_comment\\'].astype(str)\\n\\n# One Hot Encoding\\nonehot_vectorizer = CountVectorizer(binary=True)\\nX_onehot = onehot_vectorizer.fit_transform(data[\\'processed_comment_str\\'])\\n\\n# Term Frequency\\ntf_vectorizer = CountVectorizer()\\nX_tf = tf_vectorizer.fit_transform(data[\\'processed_comment_str\\'])\\n\\n# TF-IDF Vectorization\\ntfidf_vectorizer = TfidfVectorizer(max_features=10000)\\nX_tfidf = tfidf_vectorizer.fit_transform(data[\\'processed_comment_str\\'])\\n\\n# Word2Vec Embeddings with TensorFlow\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(data[\\'processed_comment_str\\'])\\nword_index = tokenizer.word_index\\n\\n# Convert text to sequences\\nsequences = tokenizer.texts_to_sequences(data[\\'processed_comment_str\\'])\\n\\n# Pad sequences\\nmaxlen = 100\\nX_padded = pad_sequences(sequences, maxlen=maxlen)\\n\\n# Train Word2Vec-like embedding layer\\nembedding_dim = 100\\nembedding_layer = tf.keras.layers.Embedding(\\n    input_dim=len(word_index) + 1,\\n    output_dim=embedding_dim\\n)\\n\\n# Get Word2Vec embeddings\\ninput_sequences = tf.convert_to_tensor(X_padded)\\nembeddings = embedding_layer(input_sequences)\\n\\n# Average embeddings to get sentence vectors\\nX_word2vec = tf.reduce_mean(embeddings, axis=1).numpy()\\n\\n# Label Encoding\\nlabel_encoder = LabelEncoder()\\ny = label_encoder.fit_transform(data[\\'label\\'])\\n\\n# Train-test split function\\ndef train_test_split_data(X, y):\\n    return train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Train Random Forest Model\\ndef train_evaluate_model(X_train, X_test, y_train, y_test):\\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    return classification_report(y_test, y_pred, output_dict=True)\\n\\n# Prepare data for model training\\nX_train_onehot, X_test_onehot, y_train, y_test = train_test_split_data(X_onehot, y)\\nX_train_tf, X_test_tf, y_train, y_test = train_test_split_data(X_tf, y)\\nX_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split_data(X_tfidf, y)\\nX_train_w2v, X_test_w2v, y_train, y_test = train_test_split_data(X_word2vec, y)\\n\\n# Evaluate models\\nperformance_onehot = train_evaluate_model(X_train_onehot, X_test_onehot, y_train, y_test)\\nperformance_tf = train_evaluate_model(X_train_tf, X_test_tf, y_train, y_test)\\nperformance_tfidf = train_evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\\nperformance_w2v = train_evaluate_model(X_train_w2v, X_test_w2v, y_train, y_test)\\n\\n# Print the performance comparison for each encoding method\\nprint(\"Performance Comparison:\")\\nprint(\"One Hot Encoding Model Performance:\\n\", classification_report(y_test, model.predict(X_test_onehot)))\\nprint(\"Term Frequency Model Performance:\\n\", classification_report(y_test, model.predict(X_test_tf)))\\nprint(\"TF-IDF Model Performance:\\n\", classification_report(y_test, model.predict(X_test_tfidf)))\\nprint(\"Word2Vec Model Performance:\\n\", classification_report(y_test, model.predict(X_test_w2v)))\\n\\n# Select the best encoding method based on F1-score\\nperformance = {\\n    \"One Hot Encoding\": performance_onehot[\\'weighted avg\\'][\\'f1-score\\'],\\n    \"Term Frequency\": performance_tf[\\'weighted avg\\'][\\'f1-score\\'],\\n    \"TF-IDF\": performance_tfidf[\\'weighted avg\\'][\\'f1-score\\'],\\n    \"Word2Vec\": performance_w2v[\\'weighted avg\\'][\\'f1-score\\']\\n}\\n\\nbest_encoding = max(performance, key=performance.get)\\n\\nprint(f\"The best encoding method based on F1-score is: {best_encoding}\")\\n\\n# Compare performance of each encoding method\\nimport matplotlib.pyplot as plt\\n\\nlabels = [\\'One Hot\\', \\'Term Frequency\\', \\'TF-IDF\\', \\'Word2Vec\\']\\nf1_scores = [performance_onehot[\\'weighted avg\\'][\\'f1-score\\'],\\n             performance_tf[\\'weighted avg\\'][\\'f1-score\\'],\\n             performance_tfidf[\\'weighted avg\\'][\\'f1-score\\'],\\n             performance_w2v[\\'weighted avg\\'][\\'f1-score\\']]\\n\\nplt.figure(figsize=(10, 6))\\nplt.bar(labels, f1_scores, color=[\\'blue\\', \\'green\\', \\'red\\', \\'purple\\'])\\nplt.xlabel(\\'Encoding Method\\')\\nplt.ylabel(\\'F1-score\\')\\nplt.title(\\'Comparison of Encoding Methods for Sarcasm Detection\\')\\nplt.show()'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('cleaned_balanced_dataset_FINAL.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['processed_comment'] = data['comment'].str.lower().str.replace('[^\\w\\s]', '')\n",
    "data['processed_comment_str'] = data['processed_comment'].astype(str)\n",
    "\n",
    "# One Hot Encoding\n",
    "onehot_vectorizer = CountVectorizer(binary=True)\n",
    "X_onehot = onehot_vectorizer.fit_transform(data['processed_comment_str'])\n",
    "\n",
    "# Term Frequency\n",
    "tf_vectorizer = CountVectorizer()\n",
    "X_tf = tf_vectorizer.fit_transform(data['processed_comment_str'])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['processed_comment_str'])\n",
    "\n",
    "# Word2Vec Embeddings with TensorFlow\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['processed_comment_str'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_comment_str'])\n",
    "\n",
    "# Pad sequences\n",
    "maxlen = 100\n",
    "X_padded = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Train Word2Vec-like embedding layer\n",
    "embedding_dim = 100\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_index) + 1,\n",
    "    output_dim=embedding_dim\n",
    ")\n",
    "\n",
    "# Get Word2Vec embeddings\n",
    "input_sequences = tf.convert_to_tensor(X_padded)\n",
    "embeddings = embedding_layer(input_sequences)\n",
    "\n",
    "# Average embeddings to get sentence vectors\n",
    "X_word2vec = tf.reduce_mean(embeddings, axis=1).numpy()\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split_data(X, y):\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Prepare data for model training\n",
    "X_train_onehot, X_test_onehot, y_train, y_test = train_test_split_data(X_onehot, y)\n",
    "X_train_tf, X_test_tf, y_train, y_test = train_test_split_data(X_tf, y)\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split_data(X_tfidf, y)\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split_data(X_word2vec, y)\n",
    "\n",
    "# Evaluate models\n",
    "performance_onehot = train_evaluate_model(X_train_onehot, X_test_onehot, y_train, y_test)\n",
    "performance_tf = train_evaluate_model(X_train_tf, X_test_tf, y_train, y_test)\n",
    "performance_tfidf = train_evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "performance_w2v = train_evaluate_model(X_train_w2v, X_test_w2v, y_train, y_test)\n",
    "\n",
    "# Print the performance comparison for each encoding method\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"One Hot Encoding Model Performance:\\n\", classification_report(y_test, model.predict(X_test_onehot)))\n",
    "print(\"Term Frequency Model Performance:\\n\", classification_report(y_test, model.predict(X_test_tf)))\n",
    "print(\"TF-IDF Model Performance:\\n\", classification_report(y_test, model.predict(X_test_tfidf)))\n",
    "print(\"Word2Vec Model Performance:\\n\", classification_report(y_test, model.predict(X_test_w2v)))\n",
    "\n",
    "# Select the best encoding method based on F1-score\n",
    "performance = {\n",
    "    \"One Hot Encoding\": performance_onehot['weighted avg']['f1-score'],\n",
    "    \"Term Frequency\": performance_tf['weighted avg']['f1-score'],\n",
    "    \"TF-IDF\": performance_tfidf['weighted avg']['f1-score'],\n",
    "    \"Word2Vec\": performance_w2v['weighted avg']['f1-score']\n",
    "}\n",
    "\n",
    "best_encoding = max(performance, key=performance.get)\n",
    "\n",
    "print(f\"The best encoding method based on F1-score is: {best_encoding}\")\n",
    "\n",
    "# Compare performance of each encoding method\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['One Hot', 'Term Frequency', 'TF-IDF', 'Word2Vec']\n",
    "f1_scores = [performance_onehot['weighted avg']['f1-score'],\n",
    "             performance_tf['weighted avg']['f1-score'],\n",
    "             performance_tfidf['weighted avg']['f1-score'],\n",
    "             performance_w2v['weighted avg']['f1-score']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, f1_scores, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Encoding Method')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('Comparison of Encoding Methods for Sarcasm Detection')\n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f8a07-4d14-4db3-b565-9b9a29b92d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
