{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca493d12-eb8e-45f9-ac99-b0d045578972",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing\n",
    "Loading the Dataset:\n",
    "\n",
    "The dataset is read from a CSV file using pandas. This step loads the data into a pandas DataFrame, making it easy to manipulate and preprocess.\n",
    "Handling Missing Values:\n",
    "\n",
    "Missing values in the 'comment' column are filled with empty strings. Handling missing data is crucial because most machine learning algorithms cannot handle missing values directly. Filling with empty strings ensures that all rows have valid text data.\n",
    "Reducing Dataset Size:\n",
    "\n",
    "For memory efficiency and faster computation, a random sample of 10,000 records is taken from the dataset. Sampling is important when working with large datasets, especially during the development and experimentation phase, as it reduces the computational load and speeds up the process.\n",
    "Encoding Target Labels:\n",
    "\n",
    "The target labels are converted from categorical values to numeric values using LabelEncoder from scikit-learn. This is necessary because machine learning algorithms typically require numeric input. Encoding ensures that the target variable is in a suitable format for training the model.\n",
    "Text Vectorization using TF-IDF:\n",
    "\n",
    "The text data in the 'comment' column is vectorized using the TF-IDF (Term Frequency-Inverse Document Frequency) technique. TF-IDF transforms the text into numerical features by calculating the importance of each word in the document relative to the entire dataset. Limiting the number of features to 1000 helps balance between capturing useful information and maintaining computational efficiency.\n",
    "Train-Test Split:\n",
    "\n",
    "The dataset is split into training and test sets with an 80/20 split using the train_test_split function from scikit-learn. This split allows the model to be trained on one portion of the data and evaluated on another, ensuring that the performance metrics reflect the model's ability to generalize to unseen data.\n",
    "Standardizing the Data:\n",
    "\n",
    "Standardization scales the features to have zero mean and unit variance using StandardScaler from scikit-learn. This step is important for gradient-based optimization methods used in neural networks, as it helps in faster convergence and more stable training.\n",
    "Building and Optimizing the CNN Model\n",
    "Defining the Hypermodel:\n",
    "\n",
    "A hypermodel is defined to specify the architecture of the CNN model and includes tunable hyperparameters. This approach allows us to explore different configurations of the model to find the one that performs best.\n",
    "Input Layer:\n",
    "\n",
    "The input layer defines the shape of the input data, which in this case is (number of features, 1). The number of features corresponds to the number of TF-IDF features (1000), and 1 represents the sequence length. This layer ensures that the data fed into the model has the correct dimensions.\n",
    "Conv1D Layer:\n",
    "\n",
    "The initial Conv1D (1-dimensional convolutional) layer is added with a tunable number of filters and kernel size. Conv1D layers are effective for extracting local features from the input sequences. The filters and kernel size are chosen from a range of values to find the best configuration.\n",
    "Flatten Layer:\n",
    "\n",
    "A Flatten layer is added to convert the multi-dimensional output of the Conv1D layer into a 1-dimensional tensor. This prepares the data for the fully connected Dense layers.\n",
    "Dense Layers:\n",
    "\n",
    "Dense layers are added with tunable units and dropout rates. These layers add non-linearity to the model and help capture complex patterns in the data. Dropout layers are used to prevent overfitting by randomly setting a fraction of the input units to zero during training. Batch normalization layers are added to stabilize and accelerate training.\n",
    "Output Layer:\n",
    "\n",
    "A final Dense layer with a single unit and sigmoid activation is added. The sigmoid activation function outputs a probability between 0 and 1, making it suitable for binary classification tasks. This layer produces the final prediction of the model.\n",
    "Compiling the Model:\n",
    "\n",
    "The model is compiled using the Adam optimizer and binary cross-entropy loss. The Adam optimizer is an adaptive learning rate optimization algorithm that is well-suited for training deep learning models. Binary cross-entropy is used as the loss function because it is appropriate for binary classification problems. The model's performance is evaluated using accuracy as a metric.\n",
    "Hyperparameter Tuning with Keras Tuner\n",
    "Initializing the Tuner:\n",
    "\n",
    "Keras Tuner's RandomSearch method is used to initialize the tuner. This method searches for the best hyperparameters by randomly sampling from the specified search space. It optimizes the model's validation accuracy over multiple trials.\n",
    "Hyperparameter Search:\n",
    "\n",
    "The tuner searches for the best hyperparameters by training the model with different configurations and evaluating their performance. It performs up to 10 trials, each executed 3 times to ensure robustness. A validation split of 20% is used to assess the model's performance on unseen data during the tuning process.\n",
    "Retrieving Best Hyperparameters:\n",
    "\n",
    "The best hyperparameters found by the tuner are retrieved. These optimal hyperparameters are then used to build the final model. This step ensures that the model configuration selected has the best chance of achieving high performance on the task.\n",
    "Training the Optimized Model\n",
    "Building the Optimized Model:\n",
    "\n",
    "The model is built using the optimal hyperparameters obtained from the tuning process. This model configuration is expected to perform better than random or manually selected configurations.\n",
    "Early Stopping:\n",
    "\n",
    "Early stopping is configured to monitor the validation loss and stop training if the loss does not improve for 3 consecutive epochs. This prevents the model from overfitting to the training data by stopping the training process once it stops improving on the validation set. The best weights are restored at the end of training.\n",
    "Learning Rate Scheduler:\n",
    "\n",
    "A learning rate scheduler is defined to adjust the learning rate during training. Initially, the learning rate is kept constant, and after a specified number of epochs, it is decreased exponentially. This helps the model converge faster and escape local minima by allowing larger steps initially and smaller, more precise steps later.\n",
    "Reshaping Data for CNN Input:\n",
    "\n",
    "The data is reshaped to match the input shape required by the Conv1D layer, which is (batch_size, sequence_length, num_features). This step ensures that the input data has the correct dimensions for the Conv1D layers to process.\n",
    "Model Training:\n",
    "\n",
    "The model is trained on the training data for up to 50 epochs, with a validation split of 20%. The early stopping and learning rate scheduler callbacks are used during training to improve efficiency and prevent overfitting. A batch size of 32 is chosen as it is a common size that balances training speed and stability.\n",
    "Evaluating the Model\n",
    "Making Predictions:\n",
    "\n",
    "Predictions are made on the test set using the trained model. The predicted probabilities are converted to binary values (0 or 1) using a threshold of 0.5. This step provides the model's final predictions for the test set.\n",
    "Calculating the F1 Score:\n",
    "\n",
    "The F1 score is calculated to evaluate the model's performance. The F1 score is the harmonic mean of precision and recall, and it provides a balanced measure of the model's accuracy, especially useful for imbalanced datasets. A high F1 score indicates that the model performs well in terms of both precision and recall.\n",
    "Making Predictions with New Input\n",
    "Function for Preprocessing and Predicting New Input:\n",
    "\n",
    "A function is defined to preprocess new text input and make predictions using the trained model. This function vectorizes the input text using the TF-IDF vectorizer, standardizes the vector, reshapes it to match the CNN input shape, and then feeds it into the model to obtain the prediction.\n",
    "Example Usage for New Input:\n",
    "\n",
    "An example is provided where a new comment is passed to the preprocessing and prediction function. The function returns the predicted class (0 or 1) for the new comment. This demonstrates how the model can be used in real-world applications to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95efd62-7eeb-4dc1-a2e5-7d6feca50e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 02m 21s]\n",
      "val_accuracy: 0.6272916793823242\n",
      "\n",
      "Best val_accuracy So Far: 0.6272916793823242\n",
      "Total elapsed time: 01h 19m 51s\n",
      "Epoch 1/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.5431 - loss: 0.9230 - val_accuracy: 0.6300 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.6544 - loss: 0.6056 - val_accuracy: 0.6156 - val_loss: 0.6807 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7104 - loss: 0.5573 - val_accuracy: 0.6275 - val_loss: 0.6970 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7432 - loss: 0.5171 - val_accuracy: 0.6062 - val_loss: 0.7481 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7576 - loss: 0.4953 - val_accuracy: 0.6212 - val_loss: 0.7154 - learning_rate: 0.0010\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "CNN F1 Score: 0.6287992027902342\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Prediction for new comment with CNN model: 0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('kaggle_train.csv')\n",
    "\n",
    "# Handle missing values in the 'comment' column\n",
    "data['comment'].fillna('', inplace=True)\n",
    "\n",
    "# Reduce dataset size for memory efficiency (sample 10,000 records)\n",
    "data = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Encode target labels if necessary\n",
    "label_column = 'label'\n",
    "label_encoder = LabelEncoder()\n",
    "data[label_column] = label_encoder.fit_transform(data[label_column])\n",
    "\n",
    "# Text Vectorization using TF-IDF with fewer features\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(data['comment']).toarray()\n",
    "\n",
    "# Split data into features and target\n",
    "y = data[label_column]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the hypermodel for CNN\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "    \n",
    "    # Conv1D layer\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]),\n",
    "                     activation='relu'))\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layers with dropout and batch normalization\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'dense_units_{i}', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner for CNN model\n",
    "tuner_cnn = kt.RandomSearch(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='hyperband_cnn',\n",
    "    project_name='cnn_optimization'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner_cnn.search(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps_cnn = tuner_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model_cnn = build_cnn_model(best_hps_cnn)\n",
    "\n",
    "# Define early stopping and learning rate scheduler\n",
    "early_stopping_cnn = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "def scheduler_cnn(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler_cnn = LearningRateScheduler(scheduler_cnn)\n",
    "\n",
    "# Reshape the data for CNN input\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Train the CNN model\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping_cnn, lr_scheduler_cnn]\n",
    ")\n",
    "\n",
    "# Predict on the test set for CNN model\n",
    "y_pred_cnn = (model_cnn.predict(X_test_cnn) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate the F1 score for CNN model\n",
    "f1_cnn = f1_score(y_test, y_pred_cnn)\n",
    "print(f\"CNN F1 Score: {f1_cnn}\")\n",
    "\n",
    "# Function to preprocess and predict new input for CNN model\n",
    "def preprocess_and_predict_cnn(comment):\n",
    "    # Preprocess the input comment\n",
    "    input_vector = tfidf.transform([comment]).toarray()\n",
    "    input_vector = scaler.transform(input_vector)\n",
    "    input_vector = input_vector.reshape(input_vector.shape[0], input_vector.shape[1], 1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = (model_cnn.predict(input_vector) > 0.5).astype(\"int32\")\n",
    "    return prediction\n",
    "\n",
    "# Example usage for new input with CNN model\n",
    "new_comment_cnn = \"This is a sample comment for prediction.\"\n",
    "prediction_cnn = preprocess_and_predict_cnn(new_comment_cnn)\n",
    "print(f\"Prediction for new comment with CNN model: {prediction_cnn[0][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
